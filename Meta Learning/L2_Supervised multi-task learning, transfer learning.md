## [ğŸ“” L2_Supervised multi-task learning, transfer learning]
âœ“ [lecture video](https://youtu.be/6stKGH6zI8g)


#### Multi-task classification vs. Multi-label learning
> '**Task**' Definition: <img src="https://user-images.githubusercontent.com/33504288/124423334-dfd8d480-dd9f-11eb-862b-45f6ef9d8db6.png" width="200" height="30">
> > **p(x)**: inputì˜ distribution, **p(y|x)**: xì˜ label distribution, **L**: loss
> 
> - **Multi-task classification**: ëª¨ë“  taskì˜ loss function(**L**)ì´ ê°™ìŒ (e.g. per-language handwriting recognition)
> - **Multi-label learning**: ëª¨ë“  taskì˜ loss function(**L**)ê³¼ input data(**p(x)**)ê°€ ê°™ìŒ (e.g. scene understanding)

> ê·¸ëŸ¼ loss functionì´ ê°™ì€ ê²½ìš°ëŠ”?
> <br>: ì´ì‚°/ì—°ì† labelì´ ì„ì—¬ìˆì„ ë•Œ, ë‹¤ë¥¸ taskë³´ë‹¤ íŠ¹ì • task í•˜ë‚˜ì— ë” ì§‘ì¤‘í•  ë•Œ

<br>

#### Basic Structure & Task Descriptor
 <img src="https://user-images.githubusercontent.com/33504288/124550449-9e156000-de6b-11eb-8f9c-d46bb4e73b0e.png" width="480" height="220"> <img src="https://user-images.githubusercontent.com/33504288/124551325-eaad6b00-de6c-11eb-87a9-2fe201f71e0d.png" width="200" height="60">


> - Basic structure: input xì™€ **task descriptorì¸ Zi**ê°€ ê°™ì´ ì£¼ì–´ì§
> > **task descriptor**ë€?<br>
> > : ë‹¨ìˆœíˆ task indexë¥¼ one-hot encodingí•œ ê²ƒ<br>
> > : ë˜ëŠ”! **meta data**ë¥¼ ì œê³µí•˜ëŠ” ê²ƒ (e.g. personalization(user features/attributes), language description of the task, formal specifications of the task)
> <br>
> ğŸ’¡ ì, ê·¸ëŸ¼ Multi-task learningì—ì„œ ìš°ë¦¬ì˜ ëª©í‘œëŠ” ë‘ ê°€ì§€!<br>
> Q1. task descriptor Zië¥¼ ì–´ë–»ê²Œ conditioning í•  ê²ƒì¸ê°€?<br>
> Q2. objectiveë¥¼ ì–´ë–»ê²Œ optimizeí•  ê²ƒì¸ê°€?

<br>

#### Conditioning
> 'How to condition on Zi'  ==  'How & Where to share parameters'
> ì „ì²´ parameter(Î¸)ë¥¼ shared parameter(Î¸sh)ì™€ task-specific parameter(Î¸i)ë¡œ ë‚˜ëˆ„ì!<br>
> objective: <img src="https://user-images.githubusercontent.com/33504288/124554096-88566980-de70-11eb-8c45-725e91adb10d.png" width="200" height="60">
> > **shared parameter**: networkì—ì„œ task descriptor ë°”ë¡œ ë’¤ì— ì˜¤ëŠ” parameter

> - **Conditioning ì¢…ë¥˜** <br>
> *ì—¬ê¸°ì„œ task descriptorëŠ” one-hot encoding vector* <br>
> <img src="https://user-images.githubusercontent.com/33504288/124559853-1df4f780-de77-11eb-8b71-9dc315f96585.png" width="750" height="150"> <br>
> > But, ë‘ ë°©ë²•ì€ ì‚¬ì‹¤ ê°™ë‹¤ëŠ” ì ...! <br>
> > <img src="https://user-images.githubusercontent.com/33504288/124558210-51368700-de75-11eb-8aa7-0b8d320bcc3c.png" width="320" height="150"> <br>
> 
> <img src="https://user-images.githubusercontent.com/33504288/124560095-66acb080-de77-11eb-838c-8103401a8410.png" width="650" height="180"> <br>
> 
> ì•ˆíƒ€ê¹ê²Œë„ ì´ëŸ° ë°©ë²•ë“¤ì€ **problem dependent**í•˜ê³ , ë¬¸ì œì— ëŒ€í•œ **intuitionì´ë‚˜ ì§€ì‹**ì— largely guided ë¨

<br>

#### Challenges in Multi-task learning
> - **Negative transfer**: independent networkê°€ ì„±ëŠ¥ì´ ë” ì¢‹ì€ ê²½ìš°
> > '**optimization challenges**' & '**limited representational capacity**'ë•Œë¬¸! <br>
> > : í•œ taskì— ëŒ€í•œ gradientê°€ ë‹¤ë¥¸ taskì˜ gradient ë•Œë¬¸ì— ë³€í•œë‹¤ê±°ë‚˜, ê° taskê°€ ë‹¤ë¥¸ ë¹„ìœ¨(ì†ë„)ë¡œ í•™ìŠµí•˜ëŠ” ê²½ìš°
> 
> negative transferê°€ ë°œìƒí–ˆì„ ë•Œ, task ê°„ì˜ **shareë¥¼ ì¤„ì´ì**! <br>
> <img src="https://user-images.githubusercontent.com/33504288/124566083-ad9da480-de7d-11eb-9dbe-5e30c817b693.png" width="240" height="70"> <br>
> 'soft parameter sharing' term: ì„œë¡œ ë‹¤ë¥¸ taskì˜ task-specific parameterë¥¼ ë¹„ìŠ·í•˜ê²Œ ë§Œë“¤ì–´ì¤Œ 
> > ğŸ‘ğŸ» ê²°ê³¼ì ìœ¼ë¡œ ë” ë§ì€ ì–‘ì˜ parameter sharingì´ ê°€ëŠ¥í•´ì§! <br>
> > ğŸ‘ğŸ» ì´ê±´ ë˜ ë‹¤ë¥¸ design decisions/hyperparameterì´ ë¨... <br>

> - **Overffiting**
> > ëœ shareí•´ì„œ ë°œìƒ => **ë” share**í•˜ë©´ ë¨!

<br>

#### + Case study: [Make recommendations for YouTube](https://dl.acm.org/doi/pdf/10.1145/3298689.3346997?casa_token=5I-bu7KVMXkAAAAA:rZiBs9D3FrXAUzL3E11jcegSnScsc_lGLb8m9WSIiNLDqZ7kBrhw5ILECWQJ-zPxZmRhlqtgpZN1)
______
#### Meta-Learning
> - **Mechanistic** view
> > : network ëª¨ë¸ì´ ì „ì²´ dataset ì½ê³ , trainingì— meta-dataset(ê°ê¸° ë‹¤ë¥¸ taskì— ëŒ€í•œ ì—¬ëŸ¬ datasetìœ¼ë¡œ êµ¬ì„±) ì‚¬ìš© <br>
> > ğŸŒŸ ì´ ê´€ì ì€ meta-learning algoë¥¼ ìˆ˜ì›”í•˜ê²Œ ì‹¤í—˜í•˜ë„ë¡ í•¨!
> 
> - **Probabilistic** view
> > : set of taskì—ì„œ ì‚¬ì „ì •ë³´ë¥¼ ì¶”ì¶œí•˜ê³ , ì´ ì‚¬ì „ì •ë³´ì™€ (small) training setì„ ì‚¬ìš©í•´ì„œ ì‚¬í›„ parameter ì¶”ë¡  <br>
> > ğŸŒŸ ì´ ê´€ì ì€ meta-learning algoë¥¼ ìˆ˜ì›”í•˜ê²Œ ì´í•´í•˜ë„ë¡ í•¨!

<br>

#### Problem definition
  <img src="https://user-images.githubusercontent.com/33504288/124573939-f60c9080-de84-11eb-982b-b0269c369fed.png" width="550" height="200">

> ğŸ’¡ ì´ ë•Œ, datasetì´ ì‘ë‹¤ë©´...? **additional data**ë¥¼(meta-train data) ì¶”ê°€í•  ìˆœ ì—†ì„ê¹Œ? <br>
> <img src="https://user-images.githubusercontent.com/33504288/124576012-dd9d7580-de86-11eb-917d-b7f2013159c1.png" width="480" height="240">

> ê·¼ë°, new taskë¥¼ í•™ìŠµí•  ë•Œë§ˆë‹¤ prior informationì„ ì“°ê³  ì‹¶ì§„ ì•Šì€ë°... <br>
> ğŸ’¡ ê·¸ëŸ¼ meta-train dataë¥¼ **meta-parameter**ë¡œ ë°”ê¾¸ì!
> <img src="https://user-images.githubusercontent.com/33504288/124577646-6d8fef00-de88-11eb-972e-95d313ea54de.png" width="700" height="180"> <br>
> - meta-training dataë¡œ meta-parameterë¥¼ êµ¬í•˜ê³ , dataì™€ meta-parameterë¡œ task-parameterë¥¼ ì˜ˆì¸¡í•¨
> > ì´ ë•Œ, task-parameterì™€ meta-train dataëŠ” ì¡°ê±´ë¶€ ë…ë¦½, meta-parameter ê¸°ë°˜
>
> ìœ„ ì‹ì€ (ì•„ì£¼ ëŒ€ëµì ì¸ ê·¼ì‚¬ì¹˜ì§€ë§Œ)<img src="https://user-images.githubusercontent.com/33504288/124602391-c15c0180-dea3-11eb-9138-0ef24025b2a5.png" width="210" height="20">ë¡œ í‘œí˜„ ê°€ëŠ¥
> > ì˜¤ë¥¸ìª½ term: **meta-training**ê³¼ ëŒ€ì‘(ì£¼ì–´ì§„ meta-training dataì—ì„œ set of meta-parametersë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒì´ ëª©í‘œ) <br>
> > ì™¼ìª½ term: **adaptation**ê³¼ ëŒ€ì‘(ìƒˆ taskì˜ dataì™€ meta-parameterê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ìƒˆ taskë¥¼ ìœ„í•œ ìƒˆ parameterë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒì´ ëª©í‘œ)


> ì´í•´ë¥¼ ìœ„í•œ simple example <br>
> <img src="https://user-images.githubusercontent.com/33504288/124606137-8d82db00-dea7-11eb-9b29-4e3ba1400df7.png" width="550" height="250"> <br>
> - **test**: test datapointì— ëŒ€í•œ predictionì„ ìœ„í•´ Î¦* ë¥¼ ì˜ˆì¸¡
> > ì´ ê±¸ meta-trainingì—ì„œë„ ë˜‘ê°™ì´ í•˜ê³  ì‹¶ìœ¼ì‹œë‹¤ëŠ” ê±°ì§€~ (ìœ„ì˜ RNNì„ trainì‹œì¼œì„œ) <br>
> > adaptation: (meta)test-timeìœ¼ë¡œ ë³¼ ìˆ˜ ìˆìŒ(meta-trainì´ ì„ í–‰ëœë‹¤ë©´)
> 
> - ê·¸ëŸ¼ (meta)training-time ë•Œ ì“°ì´ëŠ” **test datapoints**ë“¤ì€ ì–´ë””ì„œ ì˜¤ëŠ” ê±°...?
> > training/test setì€ meta-training datasetì—ì„œ! <br>
> > ê·¸ë¦¬ê³  meta test-timeì—ëŠ” ìƒˆ taskë¥¼ ë°›ê² ì£ !(meta-test setìœ¼ë¡œ trainí•˜ê¸° ì‹œë )
>
> â™¨ï¸ ì—¬ê¸°ì„œì˜ key idea(principal rule of mata-learning)<br>
> : **test and train conditions must match**
> > test ë•Œ í•™ìŠµí•  ìˆ˜ ìˆê²Œ í•˜ë ¤ë©´, meta-learning ì¤‘ì— í•™ìŠµí•  ìˆ˜ ìˆê²Œ í•´ì•¼ í•œë‹¤ëŠ” ë§!(**learning how to learn**)

â¤ï¸ ê²°ë¡ : Î¸(meta-parameter)ê°€ taskë“¤ì— ê±¸ì³ sharedëœ prior informationì´ë‹¤! parameter Î¸* ë¥¼ ê°€ì§„ modelì„ í•™ìŠµí•´ì„œ ì—¬ëŸ¬ taskë“¤ì„ í•´ê²°í•˜ì!
