## [ğŸ“” L1_Course_Introduction]

#### Multi-Modal vs. Multi-Medium
> - **Modality**
> : (ì´ë¯¸ ì¼ì–´ë‚¬ê±°ë‚˜ ê²½í—˜ëœ) íŠ¹ì • íƒ€ì…ì˜ ì •ë³´ or ì •ë³´ê°€ ì €ì¥ë˜ëŠ” representation format
> <br>(e.g. Natural language, Visual, Auditory, Haptics/touch, Smell, taste, 
> self-motion, Physiological signals, etc)
> <br> - **Medium**
> : ì •ë³´ ì €ì¥ or í†µì‹ ì„ ìœ„í•œ ìˆ˜ë‹¨/ë„êµ¬

<br>

#### 4 eras of Multimodal research
> 1. The **"behavioral"** era (1970s ~ late 1980s): McGurk effect ë°œê²¬...!(í•˜ë‚˜ì˜ modalityë§Œìœ¼ë¡œëŠ” ìª¼ê¼¼ í˜ë“¤ê² ë‹¤~)
> 2. The **"computational"** era (late 1980s ~ 2000): Audio-Visual Speech Recognition, Multimodal/Multisensory interfaces, Multimedia Computing
> 3. The **"interaction"** era (2000 ~ 2010): Modeling Human Multimodal Interaction(e.g. Siri)
> 4. The **"deep learning"** era (2010s ~): Representation learning(New large-scale multimodal datasets, improvement of computing power, high-level visual features, "dimensional" linguistic features)

<br>

#### Core Technical Challenges
> <img src="https://user-images.githubusercontent.com/33504288/124376247-04ba4280-dce1-11eb-8cc4-e4ff031e4d47.png" width="500" height="250">
> 
> _**Representation** ë½‘ì•„ì„œ **Alignment**í•˜ê³  **Translation** or **Fusion**í•¨. (+uni-modalì˜ trainingì—ì„œ ë‹¤ë¥¸ modal ì¶”ê°€ë¡œ ì‚¬ìš©í•˜ëŠ” **Co-learning**)_ 
> > ì—¬ê¸°ì„œ Translationê³¼ Fusionì€ loss functionì„ ë­˜ ì“°ëŠëƒ~ ì˜ ì°¨ì´
> > - translation: í•˜ë‚˜ì˜ modal ì •ë³´ë¥¼ ë‹¤ë¥¸ modalë¡œ ë°”ê¾¸ê¸°(e.g. image captioning)
> > - fusion: ëª¨ë“  modalì„ ì‚¬ìš©í•´ì„œ high-levelì˜ ë¬´ì–¸ê°€ë¡œ ì¶”ë¡ (e.g. emotion recognition, detecting an event in a video)

<br>

#### ğŸŒŸ **Representation**: multimodal dataë¥¼ ì–´ë–»ê²Œ í‘œí˜„í•˜ê³  ìš”ì•½í•˜ì§€?
 <img src="https://user-images.githubusercontent.com/33504288/124375456-3c26f000-dcdd-11eb-80f4-460370c3f7cc.png" width="500" height="140">

> - **Joint representations**<br>
> : multimodal representationì„ concatí•´ì„œ ì‚¬ìš©
> - **Coordinated representations**<br>
> : ê° modalì˜ representationì—ì„œ í•„ìš”í•œ ë¶€ë¶„ë“¤ë§Œ ë½‘ì•„ì„œ ì‚¬ìš©

<br>

#### ğŸŒŸ **Alignment**:ë‹¤ë¥¸ modalì—ì„œ ë‚˜ì˜¨ (sub)elenment ê°„ì˜ ì§ì ‘ì ì¸ ê´€ê³„ ì‹ë³„
 <img src="https://user-images.githubusercontent.com/33504288/124375628-31208f80-dcde-11eb-898e-4da368a0f541.png" width="550" height="170">
 
> - **Explicit Alignment**<br>
> : modal ê°„ ì§ì ‘ì ìœ¼ë¡œ ëŒ€ì‘ë˜ëŠ” ë‚´ìš© matching(audio:"ì‚¬ê³¼" -> image:ì‚¬ê³¼ ì‚¬ì§„)
> - **Implicit Alignment**<br>
> : ë‹¤ë¥¸ ë¬¸ì œë¥¼ ë” ì˜ í’€ê¸° ìœ„í•´ ë‚´ë¶€ì ìœ¼ë¡œ ì ì¬ëœ(latent) alignment ì‚¬ìš©(Attention ëª¨ë¸ì´ë‘ ë¹„ìŠ·!)
> <img src="https://user-images.githubusercontent.com/33504288/124376074-2c5cdb00-dce0-11eb-8783-e7c9c4c7fa1f.png" width="500" height="150">

<br>

#### ğŸŒŸ **Translation**: ë‹¤ë¥¸ modal í˜•ì‹ìœ¼ë¡œ dataë¥¼ ë°”ê¾¸ì!
<img src="https://user-images.githubusercontent.com/33504288/124376699-37653a80-dce3-11eb-8c1f-8819d73068d3.png" width="550" height="200">

<br>

#### ğŸŒŸ **Fusion**: predictionì„ ìœ„í•´ ë‘ ê°œ ì´ìƒì˜ moidal infoë¥¼ ê²°í•©
<img src="https://user-images.githubusercontent.com/33504288/124377360-86f93580-dce6-11eb-95ac-898e362e9c2d.png" width="500" height="200">

> - **Early fusion**<br>
> : multimodalì´ low-levelì¼ ë•Œ ì‚¬ìš©! ë¨¼ì € fusioní•˜ì§€ ì•Šìœ¼ë©´ trainingì„ í•  ìˆ˜ ì—†ì„ ë•Œ...  
> - **Late fusion**<br>
> : fusion ì „ì— ë‚´ë¶€ì ìœ¼ë¡œ ì–´ë– í•œ processingì´ í•„ìš”í•  ë•Œ ì‚¬ìš©
<img src="https://user-images.githubusercontent.com/33504288/124378638-70a2a800-dced-11eb-9643-2051db7b6948.png" width="300" height="140">

> ìš”ì¦˜ì€ ë” ë‹¤ì–‘í•œ approachë“¤ì´ ë§ë‹µë‹ˆë‹¹~

<br>

#### ğŸŒŸ **Co-Learning**: representation, ì˜ˆì¸¡ëª¨ë¸ì„ í¬í•¨í•´ì„œ modal ê°„ì˜ ì§€ì‹ì„ ì „ë‹¬
 ![image](https://user-images.githubusercontent.com/33504288/124377771-ae510200-dce8-11eb-9982-88ea2d33be7d.png)
> e.g. object detectionì„ í•˜ê³  ì‹¶ì€ë°, training ë•Œ ë‹¤ë¥¸ modalityë¥¼ ê°™ì´ ì‚¬ìš©í•˜ë©´ ë” ë„ì›€ë  ê²ƒ ê°™ì€ë°?! dataê°€ ë³„ë¡œ ì—†ëŠ”ë° ë‹¤ë¥¸ modalityì˜ dataë¥¼ ê°™ì´ ì‚¬ìš©í•˜ë©´?!
> - Parallel<br>
> : ì„œë¡œ ë‹¤ë¥¸ modal ê°„ ê±°ì˜ ëª¨ë“  dataì— ìƒì‘í•˜ëŠ” dataê°€ ìˆëŠ” ê²½ìš°
> - Non-Parallel<br>
> : ì„œë¡œ ë‹¤ë¥¸ modalì˜ data ê°„ ì •í™•íˆ ìƒì‘í•˜ëŠ” dataëŠ” ëª¨ë¥´ì§€ë§Œ, ì„œë¡œ ê´€ê³„ê°€ ìˆì„ ê²½ìš°(french sentences corpus & english sentences corpus)

 ![image](https://user-images.githubusercontent.com/33504288/124378095-90849c80-dcea-11eb-819b-c780b96d29dd.png)
> - (traslationì˜ ideaë¥¼ ì´ìš©) verbal modalì„ **intermediate representation**(co-learning representation)ìœ¼ë¡œ translateí•˜ê³ , ì´ë¥¼ ì´ìš©í•´ì„œ ìµœì¢… visualì„ generate!<br>
> - ì´ ë•Œ, cyclic lossë¥¼ ì´ìš©í•´ì„œ, verbal->visualì—ì„œ ì´ìš©í•œ ì •ë³´ë¥¼ visual->verbalë¡œ regenerateí•˜ëŠ” ë° ì‚¬ìš©í•¨. (intermediate representationìœ¼ë¡œ verbal dataì˜ ëª¨ë“  ì •ë³´ë¥¼ ê°€ì ¸ê°€ê² ë‹¤!)<br>
> - í•œ ë°©í–¥ìœ¼ë¡œë§Œ ì§„í–‰í•œë‹¤ë©´ intermediate representationì€ source data(input)ë³´ë‹¤ targetì— í›¨ì”¬ ë” ê°€ê¹ê² ì§€ë§Œ, cyclicì„ ì¶”ê°€í•¨ìœ¼ë¡œì¨ inputê³¼ë„ ë¹„ìŠ·í•´ì§.

 ![image](https://user-images.githubusercontent.com/33504288/124378421-3389e600-dcec-11eb-8006-f823132d165b.png)
> Test ë•ŒëŠ” í•˜ë‚˜ì˜ modalë§Œ ì‚¬ìš©(verbal)í•´ì„œ ê°ì • ì˜ˆì¸¡ (ë†€ëê²Œë„ train&testì— multimodalì„ ì‚¬ìš©í•œ ê²ƒê³¼ ì„±ëŠ¥ì´ ë¹„ìŠ·í•˜ë‹¤ê³  í•¨)
