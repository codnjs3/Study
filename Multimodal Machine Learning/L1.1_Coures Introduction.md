## [ðŸ“” L1_Course_Introduction]

#### Multi-Modal vs. Multi-Medium
> - **Modality**
> : (ì´ë¯¸ ì¼ì–´ë‚¬ê±°ë‚˜ ê²½í—˜ëœ) íŠ¹ì • íƒ€ìž…ì˜ ì •ë³´ or ì •ë³´ê°€ ì €ìž¥ë˜ëŠ” representation format
> <br>(e.g. Natural language, Visual, Auditory, Haptics/touch, Smell, taste, 
> self-motion, Physiological signals, etc)
> <br> - **Medium**
> : ì •ë³´ ì €ìž¥ or í†µì‹ ì„ ìœ„í•œ ìˆ˜ë‹¨/ë„êµ¬

#### 4 eras of Multimodal research
> 1. The **"behavioral"** era (1970s ~ late 1980s): McGurk effect ë°œê²¬...!(í•˜ë‚˜ì˜ modalityë§Œìœ¼ë¡œëŠ” ìª¼ê¼¼ íž˜ë“¤ê² ë‹¤~)
> 2. The **"computational"** era (late 1980s ~ 2000): Audio-Visual Speech Recognition, Multimodal/Multisensory interfaces, Multimedia Computing
> 3. The **"interaction"** era (2000 ~ 2010): Modeling Human Multimodal Interaction(e.g. Siri)
> 4. The **"deep learning"** era (2010s ~): Representation learning(New large-scale multimodal datasets, improvement of computing power, high-level visual features, "dimensional" linguistic features)

#### Core Technical Challenges
> 1. Representation: multimodal dataë¥¼ ì–´ë–»ê²Œ í‘œí˜„í•˜ê³  ìš”ì•½í•˜ì§€?
> > ![image](https://user-images.githubusercontent.com/33504288/124375456-3c26f000-dcdd-11eb-80f4-460370c3f7cc.png)

