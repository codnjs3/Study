## [ðŸ“” L1_Multimodal applications and datasets]

#### Multimodal Research Tasks
![image](https://user-images.githubusercontent.com/33504288/124379253-d93f5400-dcf0-11eb-84de-03be4d9f0ba9.png)
![image](https://user-images.githubusercontent.com/33504288/124379262-e1978f00-dcf0-11eb-84f7-8b4832d5ed37.png)
> _MMML_ì´ ë‹¤ë£¨ê³  ìžˆëŠ” _7ê°€ì§€ ì£¼ìš” real world tasks_!
> - **Affect recognition**: Emotion, Personalities, Sentiment
> - **Media description**: Image and video captioning
> - **Multimodal QA**: Image and video QA, Visual reasoning
> - **Multimodal Navigation**: Language guided navigation, Autonomous driving
> - **Multimodal Dialog**: Grounded dialog
> - **Event recognition**: Action recognition, Segmentation
> - **Multimedia information retrieval**: Content based/Cross-media

<br>

#### ðŸŒˆ Affective Computing
> 
