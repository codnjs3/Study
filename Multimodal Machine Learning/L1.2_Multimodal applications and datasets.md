## [ðŸ“” L1_Multimodal applications and datasets]

#### Multimodal Research Tasks
<center><img src="https://user-images.githubusercontent.com/33504288/124379253-d93f5400-dcf0-11eb-84de-03be4d9f0ba9.png" width="400" height="200"><img src="https://user-images.githubusercontent.com/33504288/124379262-e1978f00-dcf0-11eb-84f7-8b4832d5ed37.png" width="400" height="180"></center>

> MMMLì´ ë‹¤ë£¨ê³  ìžˆëŠ” 7ê°€ì§€ ì£¼ìš” real world tasks!
> - **Affect recognition**: Emotion, Personalities, Sentiment
> - **Media description**: Image and video captioning
> - **Multimodal QA**: Image and video QA, Visual reasoning
> - **Multimodal Navigation**: Language guided navigation, Autonomous driving
> - **Multimodal Dialog**: Grounded dialog
> - **Event recognition**: Action recognition, Segmentation
> - **Multimedia information retrieval**: Content based/Cross-media

<br>

#### ðŸŒˆ Affective Computing
> 
